{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import imageio\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_color_for_id(label):\n",
    "    \"\"\"\n",
    "    Simple function that adds fixed color depending on the id\n",
    "    \"\"\"\n",
    "    palette = (2 ** 11 - 1, 2 ** 15 - 1, 2 ** 20 - 1)\n",
    "\n",
    "    color = [int((p * (label ** 2 - label + 1)) % 255) for p in palette]\n",
    "    return tuple(color)\n",
    "# plot_one_box(bboxes, overlayImage, label=label, color=color, line_thickness=line_thickness, bottom_label=bottom_label)\n",
    "def plot_one_box(x, img, color=None, label=None, line_thickness=None, bottom_label=None):\n",
    "    # Plots one bounding box on image img\n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1# line/font thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    \n",
    "    tf = max(tl - 1, 1)# font thickness\n",
    "    if label:\n",
    "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        c4 = c1[0] + t_size[0], c1[1] - t_size[1] - 3# filled\n",
    "        cv2.rectangle(img, c1, c4, color, -1, cv2.LINE_AA)\n",
    "        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "    if bottom_label:\n",
    "        a_size = cv2.getTextSize(bottom_label, 0, fontScale=tl / 4, thickness=tf)[0]\n",
    "        c3 = c1[0] + a_size[0], c2[1] + a_size[1] + 3\n",
    "        cv2.rectangle(img, (c1[0], c2[1]), c3, color, -1, cv2.LINE_AA)\n",
    "        cv2.putText(img, bottom_label, (c1[0], c2[1] + 12), 0, tl / 4, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "\n",
    "def plot_text(up_text,img,c1=(546,40),c2=(550,50),color=(0,0,255)):\n",
    "    tl = 3\n",
    "    tf = max(tl - 1, 1)# font thickness\n",
    "    up_text_size = cv2.getTextSize(up_text, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "    c4 = c1[0] + up_text_size[0], c1[1] - up_text_size[1] - 3# filled\n",
    "    cv2.rectangle(img, c1, c4, color, -1, cv2.LINE_AA)\n",
    "    cv2.putText(img, up_text, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(im, target_width = 640):\n",
    "    h,w,_  = im.shape\n",
    "    target_height = int(h / w * target_width)\n",
    "    im = cv2.resize(im , (target_width , target_height), interpolation = cv2.INTER_AREA)  \n",
    "    return im,target_height,target_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyxy_xywh(x1,y1,x2,y2):\n",
    "    w = abs(x2-x1)\n",
    "    h = abs(y2-y1)\n",
    "    x,y = x1+w/2,y1+h/2\n",
    "    return x,y,w,h\n",
    "def xywh_xyxy(x,y,w,h):\n",
    "    x1 = x-w/2\n",
    "    y1 = y-h/2\n",
    "    x2 = x+w/2\n",
    "    y2 = y+h/2\n",
    "    return int(x1),int(y1),int(x2),int(y2)\n",
    "\n",
    "def scale_con(x,y,w,h,old,new):\n",
    "    # old : tuple(height,width)\n",
    "    x = int(x* old[0]/new[0])\n",
    "    y = int(y*old[1]/new[1])\n",
    "    h = int(h*old[0]/new[0])\n",
    "    w = int(w*old[1]/new[1])\n",
    "    return x,y,w,h \n",
    "def isInPolygon(PointF, polygon):\n",
    "    i = 0\n",
    "    c = False\n",
    "    j = len(polygon) - 1\n",
    "    for i in range(len(polygon)):        \n",
    "        if (((polygon[i][1] > PointF[1]) != (polygon[j][1] > PointF[1])) and \n",
    "        (PointF[0] < (polygon[j][0] - polygon[i][0]) * (PointF[1] - polygon[i][1]) / (polygon[j][1] - polygon[i][1]) + polygon[i][0])):\n",
    "            c = not(c)\n",
    "        j = i\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ 2023-12-25 Python-3.8.3 torch-1.12.0+cu102 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 212 layers, 20852934 parameters, 0 gradients, 47.9 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load(\"./yolov5\", 'custom', path='./yolov5/runs/train/exp3/weights/best.pt', source='local')\n",
    "model.conf = 0.5\n",
    "\n",
    "# model = torch.hub.load(\"../Human_Detection/yolov5\", \"yolo5m\",source=\"local\")  # or yolov5n - yolov5x6, custom\n",
    "# '.', 'custom', path='/path/to/yolov5/runs/train/exp5/weights/best.pt', source='local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boxmot import tracker_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-12-28 07:22:40.507\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m207\u001b[0m - \u001b[32m\u001b[1mSuccessfully loaded pretrained weights from \"/media/umair/New Volume1/Object_Detection_tracking/Human_Detection_Tracking_Counting/osnet_x1_0_dukemtmcreid.pt\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Tracker Initialization\n",
    "# Tracker options: 'deepocsort, botsort, strongsort, ocsort, bytetrack')\n",
    "path = Path(\"./osnet_x1_0_dukemtmcreid.pt\").absolute()\n",
    "config = tracker_zoo.get_tracker_config('botsort')\n",
    "tracker = tracker_zoo.create_tracker('botsort', \"./boxmot/configs/botsort.yaml\", path,\n",
    "                                     \"cpu\", False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"./video/student_lobby.mp4\")\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Video writer\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output = cv2.VideoWriter('./video/student_lobby_counted.mp4', fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# Polygon Area for counting tracking\n",
    "\n",
    "\n",
    "area1 = np.array([[351,380],[302,713],[465,711],[487,395]])\n",
    "area2 = np.array([[494,395],[472,711],[608,711],[630,402]])\n",
    "\n",
    "# Polygon Detection Area\n",
    "det_roi = np.array([[341,375],[292,710],[873,710],[835,375]])\n",
    "\n",
    "left_count = []\n",
    "right_count = []\n",
    "area1_tracks = []\n",
    "area2_tracks = []\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == True:\n",
    "        \n",
    "        plot_text(\"Left Count: {}\".format(len(left_count)),frame,c1=(500,50),c2=(510,60),color=(0,0,255))\n",
    "        plot_text(\"Right Count: {}\".format(len(right_count)),frame,c1=(500,85),c2=(510,95),color=(0,0,255))\n",
    "        cv2.polylines(frame, [area1], True, (0,0,255), 2)\n",
    "        cv2.polylines(frame, [area2], True, (0,0,255), 2)\n",
    "        cv2.polylines(frame, [det_roi], True, (0,255,0), 2)\n",
    "        \n",
    "        img,res_height,res_width = resize_img(frame)\n",
    "        results = model(img)\n",
    "        preds = np.array(results.xyxy[0])\n",
    "        bboxes = []\n",
    "        \n",
    "        for ind,(x1,y1,x2,y2,conf,cls) in enumerate(preds):\n",
    "#             print(ind,x1,y1,x2,y2,conf,cls)\n",
    "            x1,y1,x2,y2 = scale_con(x1,y1,x2,y2,(frame_height,frame_width),(res_height,res_width))\n",
    "            if isInPolygon((x2,y2),det_roi):\n",
    "                bboxes.append([x1,y1,x2,y2,conf,cls])\n",
    "        \n",
    "#         print(bboxes)\n",
    "        if len(bboxes) == 0:\n",
    "            bboxes = [[0,0,0,0,0.0,0]]\n",
    "        outputs = tracker.update(np.array(bboxes),frame)\n",
    "#         print(outputs)\n",
    "        for x1,y1,x2,y2,tid,conf,_,_ in  outputs:\n",
    "            plot_one_box(x=(x1,y1,x2,y2),img=frame,color=compute_color_for_id(int(tid))\n",
    "                         ,label=str(int(tid)),line_thickness=1)\n",
    "            \n",
    "            if isInPolygon((x2,y2),area1):\n",
    "                if tid not in area1_tracks:\n",
    "                    area1_tracks.append(tid)\n",
    "                    if tid in area2_tracks:\n",
    "                        left_count.append(tid)\n",
    "            elif isInPolygon((x2,y2),area2):\n",
    "                if tid not in area2_tracks:\n",
    "                    area2_tracks.append(tid)\n",
    "                    if tid in area1_tracks:\n",
    "                        right_count.append(tid)\n",
    "                    \n",
    "            \n",
    "            \n",
    "        output.write(frame)    \n",
    "#         cv2.imshow('Frame',frame)\n",
    "#         k =cv2.waitKey(0) & 0xFF\n",
    "#         #press y to exit\n",
    "#         if k==121:\n",
    "#             break\n",
    "#         else:\n",
    "#             pass\n",
    "    else:\n",
    "        break\n",
    "cap.release() \n",
    "# cv2.destroyAllWindows()\n",
    "output.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(\"./video/pedestrian.webm\")\n",
    "# frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "# frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "# fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# # Video writer\n",
    "\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "# output = cv2.VideoWriter('./video/crowd_street.mp4', fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "# while(cap.isOpened()):\n",
    "#     ret, frame = cap.read()\n",
    "#     if ret == True:\n",
    "#         output.write(frame) \n",
    "#     else: \n",
    "#         break\n",
    "# cap.release()\n",
    "# output.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280 714 12.0\n",
      "640 357 12.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# writing processed and unprocessed video file side by side\n",
    "cap1 = cv2.VideoCapture(\"./video/student_lobby.mp4\")\n",
    "cap2 = cv2.VideoCapture(\"./video/student_lobby_counted.mp4\")\n",
    "frame_width = int(cap1.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap1.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "target_width = frame_width//2\n",
    "target_height = frame_height//2\n",
    "fps = cap1.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Video writer\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output = cv2.VideoWriter('./video/joined_video.mp4', fourcc, fps, (frame_width, target_height))\n",
    "\n",
    "\n",
    "# target_frames = 800\n",
    "print(frame_width,frame_height,fps)\n",
    "print(target_width,target_height,fps)\n",
    "while(cap1.isOpened()):\n",
    "    ret1, frame1 = cap1.read()\n",
    "    ret2, frame2 = cap2.read()\n",
    "    if ret1 == True and ret2 == True :\n",
    "        res_frame1 = cv2.resize(frame1 , (target_width , target_height), interpolation = cv2.INTER_AREA)  \n",
    "        res_frame2 = cv2.resize(frame2 , (target_width , target_height), interpolation = cv2.INTER_AREA) \n",
    "        concat_frame = np.concatenate((res_frame1, res_frame2), axis=1)\n",
    "        \n",
    "        output.write(concat_frame)\n",
    "#         cv2.imshow('Frame',concat_frame)\n",
    "#         k =cv2.waitKey(0) & 0xFF\n",
    "#         #press y to exit\n",
    "#         if k==121:\n",
    "#             break\n",
    "#         else:\n",
    "#             pass\n",
    "    else:\n",
    "        break\n",
    "cap1.release()\n",
    "cap2.release()\n",
    "cv2.destroyAllWindows()\n",
    "output.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import moviepy.editor as mp\n",
    "from moviepy.editor import VideoFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_video_to_gif(input_video_path, output_gif_path, fps=25):\n",
    "    video_clip = mp.VideoFileClip(input_video_path)\n",
    "    video_clip.write_gif(output_gif_path, fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file ./video/joined_video.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    }
   ],
   "source": [
    "video_file = \"./video/joined_video.mp4\"\n",
    "output_gif_file = \"./video/joined_video.gif\"\n",
    "convert_video_to_gif(video_file, output_gif_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_half_video_to_gif(input_video_path, output_gif_path):\n",
    "    # Load the video clip\n",
    "    video_clip = VideoFileClip(input_video_path)\n",
    "\n",
    "    # Get the duration of the video\n",
    "    total_duration = video_clip.duration\n",
    "\n",
    "    # Set the end time for the first half\n",
    "    end_time = total_duration / 2\n",
    "#     print(end_time,type(end_time))\n",
    "\n",
    "    # Trim the video to the first half\n",
    "    trimmed_clip = video_clip.subclip(0, 5.0)\n",
    "\n",
    "    # Write the trimmed video as a GIF\n",
    "    trimmed_clip.write_gif(output_gif_path)\n",
    "\n",
    "video_file = \"./video/output_crowd_street.mp4\"\n",
    "output_gif_file = \"./video/clipped_crowd_street.gif\"\n",
    "convert_half_video_to_gif(video_file, output_gif_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
